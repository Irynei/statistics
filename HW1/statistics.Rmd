---
title: "Statistics | HW1"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r}
# data preprocessing
ceodata <- read.csv('ceo.csv')
ceodata$X <- NULL
salary <- ceodata$salary
head(ceodata)
```
# Problem 1

## Task 1

### 1a
```{r}
mean(salary)
```
Mean(2027.517) - average salary.

```{r}
mean.default(salary, trim=0.1)
```
Trimmed mean(1710.092) - mean of salaries without lowest 10% and highest 10% of values.  
Used to eliminate the impact of very large or very small salaries(called outliers) on the mean.

```{r}
median(salary)
```
Median(1600) - central salary. Half of salaries are smaller than median and half are larger.


```{r}
quantile(salary, c(0.25, 0.75))
```
Lower quartile(1084.0) - 25% of salaries are smaller than lower quartile (consequently 75% are larger)  
Upper quartile(2347.5) - 75% of salaries are smaller than upper quartile (consequently 25% are larger)


```{r}
quantile(salary, c(0.1,0.9))
```
Lower 10%-quantile(750.0) - 10% of salaries are smaller than lower quartile (consequently 90% are larger)  
Upper 10%-quantile(3384.4) - 90% of salaries are smaller than upper quartile (consequently 10% are larger)



### 1b

```{r}
Fn <- ecdf(salary)
plot(Fn)
```

Empirical cumulative distribution function of salaries.

```{r}
quantile(salary, c(0.2))
quantile(salary, c(0.8))
Fn(1000)
1 - Fn(5000)
```
*
  $\hat{F}^{-1}(0.2)=976.2$ - 20% of CEOs have at most $976.2 salary.  
  $\hat{F}^{-1}(0.8)=2613$ - 80% of CEOs have at most $2613 salary.

*
  $\hat{F}(1000)=0.223$ - 22.3% of CEOs have at most $1000 salary.  
  $1 - \hat{F}(5000)=0.053$ - 5.3% of CEOs have at least $5000 salary.



### 1c

```{r}
hist(salary, col="darkolivegreen3")
```
```{r}
boxplot(salary, main="Boxplot of salary")
```

As we can see from histogram and boxplot, salary distribution is not symmetric.  
Location measures:  
**mean** is very sensitive to outliers and therefore meaningful only for symmetric data - not appropriate here.  
**trimmed mean** is much more robust to outliers compared to the simple mean - appropriate here.  
**median** is not as strongly influenced by outliers as mean - appropriate here.  
**the interquartile range** is also robust to outliers. There are at least [n/2] of all observations in the interval - appropriate here.  


```{r}
library(moments)
skewness(salary)

```
As a measure of symmetry we can use skewness.
If skewness is larger than zero, then the distribution is right-skewed, therefor salary distribution is right-skewed.



### 1d
```{r}
hist(salary, col="darkolivegreen3")
```

Histogram of salary. Default formula to compute number of bars is Sturges' formula (${\displaystyle k=\lceil \log _{2}n\rceil +1\,}$)  
For salary data $n = `r length(salary)`$, ${\displaystyle k=\lceil \log _{2}`r length(salary)`\rceil +1 = 10\,}$.


```{r}
hist(salary, breaks=4, col="darkolivegreen3")
```

Too rough histogram with only 4 bars.

```{r}
hist(salary, breaks=1000, col="darkolivegreen3")
```

Too detailed histogram with 1000 bars.

We can see that too detailed histogram shows too much individual data and we can't clearly see the underlying pattern.
On the other hand, too rough histogram has only 4 bars and again we are unable to find underlying pattern in the data.



### 1e
```{r}
hist(salary, col="darkolivegreen3")
hist(log(salary), col="tomato", main="Histogram of ln(salary)")
```

Histogram of ln(salary)

```{r}
boxplot(salary, main="Boxplot of salary",col="darkolivegreen3")
```

```{r}
boxplot(log(salary),main="Boxplot of ln(salary)", col="tomato")
```

Boxplot of ln(salary). We can see that this is almost symmetric distribution.


```{r}
mean(log(salary))
median(log(salary))
```
Mean and median of ln(salary).  
In a symmetric distribution, the mean and median fall at the same point.
As we can see mean is pretty much the same as median.



## Task 2

### 2a


```{r}
library(ggplot2)
library(reshape2)
pearson_correlations = cor(ceodata, method="pearson")
qplot(x=Var1, y=Var2, data=melt(pearson_correlations), fill=value, geom="tile") + scale_fill_gradient(low = "white", high = "red")
```

Heatmap of Pearson correlations. The more bright red square, the bigger correlation.  
For example, we can see that there are pretty strong correlation between sales and profits, which is very logical.  
Note: Pearson correlation evaluates the linear relationship between two continuous variables.

### 2b
```{r}
pairs(~salary + totcomp + tenure + age + sales + profits + assets, data=ceodata, main="CEO Scatterplot Matrix")
```

Here we can see almost linear relationship between some variables, for instance sales and profits.  
At the same time there are nonlinear correlations and no correlations.  
So I think that for some variables linear correlation coefficients are appropriate here.  

```{r}
spearman_correlation = cor(ceodata, method="spearman")
qplot(x=Var1, y=Var2, data=melt(spearman_correlation), fill=value, geom="tile") + scale_fill_gradient(low = "white", high = "red")

```

Heatmap of Spearman correlations. 
Here we can see more strong correlations between some variables than on heatmap using Pearson correlations.  
This could happen because of nonlinear but monotonic relationship between variables.  
For example, totcomp and salary or totcomp and profits.   
Note: The Spearman correlation evaluates the monotonic relationship between two continuous variables. 

```{r}
sorted_salaries = sort(ceodata$salary)
min(which(sorted_salaries==6000))
```

Min rank of salary=6000. 

### 2c

```{r}
hist(salary[which(ceodata$age > 50)],  col="darkolivegreen3", main="Age>50 and Age<50 Histograms Of Salary", xlab = "Salary")
hist(salary[which(ceodata$age < 50)],  col="tomato", add=T)
legend("topright", legend=c("Age > 50", "Age < 50"),col=c("darkolivegreen3", "tomato"), lwd=3)
```

Histograms of two groups: Age > 50 and Age < 50.  
From this plot we can see that there are far less CEOs with age < 50 comparing to CEOs with age > 50.  

```{r}
plot(ecdf(salary[which(ceodata$age > 50)]), col="darkolivegreen3", main="Salary ECDF for Age>50 and Age<50")
plot(ecdf(salary[which(ceodata$age < 50)]), add=T, col="tomato")
legend("bottomright", legend=c("Age > 50", "Age < 50"),col=c("darkolivegreen3", "tomato"), lwd=5)
```

ECDF plots of two groups: Age > 50 and Age < 50.  
The distribution is pretty much the same for small salaries.  
However, more CEO's with age > 50 get larger salaries. This can clearly be seen from ecdf plot.
For example, let's took 0.95, we can see that 95% of CEOs > 50 years old get at most `r quantile(salary[which(ceodata$age > 50)], c(0.95))`   
while 95% of CEOs < 50 years old get at most `r quantile(salary[which(ceodata$age < 50)], c(0.95))`  
 

## Task 3

### 3a
```{r}
grouped_data <- data.frame(salary=ceodata$salary, age=ceodata$age)
# group data by categories 
grouped_data$age <- ifelse(grouped_data$age < 50, "a1", "a2")
grouped_data$salary[suppressWarnings(as.integer(grouped_data$salary)) < 2000] <- "s1"
grouped_data$salary[suppressWarnings(as.integer(grouped_data$salary) >= 2000) & suppressWarnings(as.integer(grouped_data$salary)) < 4000] <- "s2"
grouped_data$salary[suppressWarnings(as.integer(grouped_data$salary)) >= 4000] <- "s3"
```
```{r}
con_table <- xtabs(~age+salary, data=grouped_data)
addmargins(con_table)
```

Contigency table with absolute frequencies.  


```{r}
con_table <- xtabs(~age+salary, data=grouped_data)
addmargins(con_table / nrow(grouped_data))
```

Contigency table with relative frequencies.  

### 3b

We can see that there are small amount of CEOs < 50 years old.  
Also, CEOs < 50 years get smaller salary comparing to CEOs > 50 years.  
We can see that there are 62 CEOs under 50 years and 52 of them have < 2000 salary.  

### 3c
```{r}
con_table <- xtabs(~age+salary, data=grouped_data)
chisq.test(con_table)
```

# 2 Problem

## Task 1

### 1a

```{r}
set.seed(19)
simulated <- rnorm(50, 10, 9)
normal <- rnorm(1000000, 10, 9)
hist(simulated, col="darkolivegreen3",prob=TRUE, xlim=c(-40, 40), main="")
lines(density(normal), col="tomato", lwd=2)
legend("topleft", legend=c("N(10,9) histogram", "N(10,9) density"),col=c("darkolivegreen3", "tomato"), lwd=3)

```

We can see that density plot of N(10, 9) is pretty the same as hist of simulated N(10,9).  
However, we have only 50 samples in simulated N(10, 9), so It's not really clear that simulated is normal distribution.  

### 1b

```{r}
set.seed(19)
normal <- rnorm(100000, 10, 9)
simulated_t5 <- rt(50, df=5)
simulated_t5 <- 10 + 3 * sqrt(3 / 5) * simulated_t5
hist(simulated_t5, col="darkolivegreen3", prob=TRUE, xlim = c(-5, 25), main="")
lines(density(normal), col="tomato", lwd=2)
legend("topleft", legend=c("transformed t5 hist", "N(10,9) density"),col=c("darkolivegreen3", "tomato"), lwd=3)
```

Here we can see hist of transformed t5 distribution and density plot of N(10, 9).  
It is obvious that transformed t5 has higher density that N(10, 9).  


## Task 2

### 2a

```{r}
set.seed(19)
normal <- rnorm(50, 10, 9)
list <- c(normal)
p <- 49
for (i in 0:49){
  list = c(list, 16 + i * (24 - 16)/p)
}
mean(normal)
mean(list)
median(normal)
median(list)
var(normal)
var(list)
boxplot(normal, main="Normal")
boxplot(list, main="Simulated with outliers")
```

Here on boxplots we can clearly see the impact of adding outliers.  
Median and mean become significantly larger.  
Variance also changes significantly.  


### 2b, 2c, 2d
Interactive graphics links:

* [Boxplot animation](https://irynei.shinyapps.io/boxplot_animation/)  
* [Histogram animation](https://irynei.shinyapps.io/hist_animation/)  

Here we can see how adding outliers changes location measures.  

## Task 3

### 3b

```{r}
u <- rnorm(50, 0, 1)
set.seed(10)
v <- rnorm(50, 0, 1)
p <- 0.7

v <- p*u + sqrt(1 - p * p) * v


```

### 3c

```{r}
set.seed(19)
u <- rnorm(10000, 0, 1)
set.seed(44)
v <- rnorm(10000, 0, 1)
p <- 0.75
v_tansformed <- p * u + sqrt(1 - p^2) * v
cor(data.frame(u, v_tansformed), method="pearson")
cor(data.frame(u, v_tansformed), method="spearman")
```

Simulated U and V*. Pearson and Spearman correlation coefficients.  

```{r}
# exp transformation
v_exp <- exp(v_tansformed)
cor(data.frame(u, v_exp), method="pearson")
cor(data.frame(u, v_exp), method="spearman")

```

Here we can see that Spearman correlation coefficients remain unchanged and Pearson correlation coefficients changed.  
This is because Spearman correlation is stable to transformations.  
